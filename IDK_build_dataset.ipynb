{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDK_build_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xsZuyTaEC92",
        "colab_type": "text"
      },
      "source": [
        "# IDK : Incomplete Data Knowledge base question answering\n",
        "\n",
        "Construction of the IDK dataset, introduced in the paper *Question Answering when Knowledge Bases are Incomplete*.\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/camillepradel/idk/blob/master/IDK_build_dataset.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/camillepradel/idk/blob/master/IDK_build_dataset.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb5V5y6m_qA6",
        "colab_type": "text"
      },
      "source": [
        "## Citing\n",
        "If you used our dataset, please kindly cite our paper\n",
        "\n",
        "```\n",
        "@inproceedings{pradel-2020-idk,\n",
        "    title = \"Question Answering when Knowledge Bases are Incomplete\",\n",
        "    author = \"Pradel, Camille and Sileo, Damien and Rodrigo, Álvaro and Peñas, Anselmo and Agirre, Eneko\",\n",
        "    booktitle = \"Proceedings of the Eleventh International Conference of the CLEF Association (CLEF 2020)\",\n",
        "    year = \"2020\"\n",
        "}\n",
        "```\n",
        "and the original work\n",
        "```\n",
        "@inproceedings{Yu&al.18c,\n",
        "  title     = {Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task},\n",
        "  author    = {Tao Yu and Rui Zhang and Kai Yang and Michihiro Yasunaga and Dongxu Wang and Zifan Li and James Ma and Irene Li and Qingning Yao and Shanelle Roman and Zilin Zhang and Dragomir Radev}\n",
        "  booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\",\n",
        "  address   = \"Brussels, Belgium\",\n",
        "  publisher = \"Association for Computational Linguistics\",\n",
        "  year      = 2018\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtI-pjuSCfGK",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3akjRI0vdWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import sqlparse\n",
        "import random\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import shutil\n",
        "from enum import Enum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIYeTwXv33MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = os.getcwd()\n",
        "spider_repo_path = f\"{base_path}/spider\"\n",
        "spider_data_path = f\"{base_path}/dataset\"\n",
        "altered_spider_data_path = f\"{base_path}/altered_dataset\"\n",
        "original_databases_path = f\"{spider_data_path}/database\"\n",
        "altered_databases_path = f\"{altered_spider_data_path}/database\"\n",
        "altered_dev_path = f\"{altered_spider_data_path}/dev.json\"\n",
        "\n",
        "dev_path = f\"{spider_data_path}/dev.json\"\n",
        "train_path = f\"{spider_data_path}/train_spider.json\"\n",
        "altered_dev_path = f\"{altered_spider_data_path}/dev.json\"\n",
        "altered_train_path = f\"{altered_spider_data_path}/train_spider.json\"\n",
        "dataset_tables = f\"{spider_data_path}/tables.json\"\n",
        "altered_dataset_tables = f\"{altered_spider_data_path}/tables.json\"\n",
        "\n",
        "spider_repo_path_bash = spider_repo_path.replace(' ', '\\\\ ')\n",
        "spider_data_path_bash = spider_data_path.replace(' ', '\\\\ ')\n",
        "altered_spider_data_path_bash = altered_spider_data_path.replace(' ', '\\\\ ')\n",
        "original_databases_path_bash = original_databases_path.replace(' ', '\\\\ ')\n",
        "altered_databases_path_bash = altered_databases_path.replace(' ', '\\\\ ')\n",
        "altered_dev_path_bash = altered_dev_path.replace(' ', '\\\\ ')\n",
        "dataset_tables_bash = f\"{spider_data_path}/tables.json\".replace(' ', '\\\\ ')\n",
        "altered_dataset_tables_bash = f\"{altered_spider_data_path}/tables.json\".replace(' ', '\\\\ ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFkkDTptO4eE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dowload Spider original dataset\n",
        "# more info about dataset at https://yale-lily.github.io/spider\n",
        "# dataset is hosted on a public google drive repo: https://drive.google.com/uc?export=download&id=11icoH_EA-NYb0OrPTdehRWm_d7-DIzWX\n",
        "# we use method from https://www.matthuisman.nz/2019/01/download-google-drive-files-wget-curl.html to download it\n",
        "%cd $base_path\n",
        "\n",
        "!curl -L -c cookies.txt 'https://docs.google.com/uc?export=download&id=11icoH_EA-NYb0OrPTdehRWm_d7-DIzWX' | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n",
        "!curl -L -b cookies.txt -o spider.zip 'https://docs.google.com/uc?export=download&id=11icoH_EA-NYb0OrPTdehRWm_d7-DIzWX&confirm='$(<confirm.txt)\n",
        "!rm -f confirm.txt cookies.txt\n",
        "\n",
        "!unzip -q spider.zip -d $spider_data_path_bash\n",
        "%mv $spider_data_path_bash/spider/* $spider_data_path_bash/\n",
        "%rm -r $spider_data_path_bash/spider\n",
        "%rm spider.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yvkjMynstVrc",
        "colab": {}
      },
      "source": [
        "# clone project and reset to given commit hash for reproducibility\n",
        "!git clone https://github.com/taoyds/spider.git $spider_repo_path_bash\n",
        "%cd $spider_repo_path_bash\n",
        "!git reset --hard 2b663fb9e77e079dd468086cbc16802fc149b36e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hPh1oOi5tVri",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "# apply necessary edits to spider/preprocess/get_tables.py:\n",
        "#  - remove unused nltk import\n",
        "#  - fix tab problems\n",
        "#  - catch \"expected\" exception\n",
        "#  - ...\n",
        "%cd $spider_repo_path_bash\n",
        "patch_file_content = \"\"\"diff --git a/preprocess/get_tables.py b/preprocess/get_tables.py\n",
        "index a8b36f0..7a7156a 100644\n",
        "--- a/preprocess/get_tables.py\n",
        "+++ b/preprocess/get_tables.py\n",
        "@@ -4,7 +4,7 @@ import json\n",
        " import sqlite3\n",
        " from os import listdir, makedirs\n",
        " from os.path import isfile, isdir, join, split, exists, splitext\n",
        "-from nltk import word_tokenize, tokenize\n",
        "+# from nltk import word_tokenize, tokenize\n",
        " import traceback\n",
        " \n",
        " EXIST = {\"atis\", \"geo\", \"advising\", \"yelp\", \"restaurants\", \"imdb\", \"academic\"}\n",
        "@@ -26,7 +26,7 @@ def convert_fk_index(data):\n",
        "             if ref_cid and cid:\n",
        "                 fk_holder.append([cid, ref_cid])\n",
        "         except:\n",
        "-\"\"\" + '\\t' + \"\"\"    traceback.print_exc()\n",
        "+            traceback.print_exc()\n",
        "             print \"table_names_original: \", data['table_names_original']\n",
        "             print \"finding tab name: \", tn, ref_tn\n",
        "             sys.exit()\n",
        "@@ -46,6 +46,7 @@ def dump_db_json_schema(db, f):\n",
        "          'column_names_original': [(-1, '*')],\n",
        "          'column_names': [(-1, '*')],\n",
        "          'column_types': ['text'],\n",
        "+         'column_uninterpreted_types': ['IGNORE'],\n",
        "          'primary_keys': [],\n",
        "          'foreign_keys': []}\n",
        " \n",
        "@@ -56,7 +57,7 @@ def dump_db_json_schema(db, f):\n",
        "         data['table_names'].append(table_name.lower().replace(\"_\", ' '))\n",
        "         fks = conn.execute(\"PRAGMA foreign_key_list('{}') \".format(table_name)).fetchall()\n",
        "         #print(\"db:{} table:{} fks:{}\".format(f,table_name,fks))\n",
        "-\"\"\" + '\\t' + \"\"\"fk_holder.extend([[(table_name, fk[3]), (fk[2], fk[4])] for fk in fks])\n",
        "+        fk_holder.extend([[(table_name, fk[3]), (fk[2], fk[4])] for fk in fks])\n",
        "         cur = conn.execute(\"PRAGMA table_info('{}') \".format(table_name))\n",
        "         for j, col in enumerate(cur.fetchall()):\n",
        "             data['column_names_original'].append((i, col[1]))\n",
        "@@ -75,7 +76,9 @@ def dump_db_json_schema(db, f):\n",
        "             else:\n",
        "                 data['column_types'].append('others')\n",
        " \n",
        "-            if col[5] == 1:\n",
        "+            data['column_uninterpreted_types'].append(col_type)\n",
        "+\n",
        "+            if col[5] >= 1:\n",
        "                 data['primary_keys'].append(len(data['column_names'])-1)\n",
        " \n",
        "     data[\"foreign_keys\"] = fk_holder\n",
        "@@ -111,15 +116,19 @@ if __name__ == '__main__':\n",
        "         db = join(input_dir, df, f)\n",
        "         print '\\\\nreading new db: ', df\n",
        "         table = dump_db_json_schema(db, df)\n",
        "-        prev_tab_num = len(ex_tabs[df][\"table_names\"])\n",
        "-        prev_col_num = len(ex_tabs[df][\"column_names\"])\n",
        "-        cur_tab_num = len(table[\"table_names\"])\n",
        "-        cur_col_num = len(table[\"column_names\"])\n",
        "-        if df in ex_tabs.keys() and prev_tab_num == cur_tab_num and prev_col_num == cur_col_num and prev_tab_num != 0 and len(ex_tabs[df][\"column_names\"]) > 1:\n",
        "-            table[\"table_names\"] = ex_tabs[df][\"table_names\"]\n",
        "-            table[\"column_names\"] = ex_tabs[df][\"column_names\"]\n",
        "-        else:\n",
        "-            print \"\\\\n----------------------------------problem db: \", df\n",
        "+        try:\n",
        "+            prev_tab_num = len(ex_tabs[df][\"table_names\"])\n",
        "+            prev_col_num = len(ex_tabs[df][\"column_names\"])\n",
        "+            cur_tab_num = len(table[\"table_names\"])\n",
        "+            cur_col_num = len(table[\"column_names\"])\n",
        "+            if df in ex_tabs.keys() and prev_tab_num == cur_tab_num and prev_col_num == cur_col_num and prev_tab_num != 0 and len(ex_tabs[df][\"column_names\"]) > 1:\n",
        "+                table[\"table_names\"] = ex_tabs[df][\"table_names\"]\n",
        "+                table[\"column_names\"] = ex_tabs[df][\"column_names\"]\n",
        "+            else:\n",
        "+                print \"\\\\n----------------------------------problem db: \", df\n",
        "+        except KeyError:\n",
        "+            # considered db was not known by previous tables file; this is not a problem\n",
        "+            pass\n",
        "         tables.append(table)\n",
        "     print \"final db num: \", len(tables)\n",
        "     with open(output_file, 'wt') as out:\n",
        "\"\"\"\n",
        "with open(\"get_tables.patch\", \"w\") as f: \n",
        "  f.write(patch_file_content)\n",
        "!git apply get_tables.patch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBMFh7itIlH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir $altered_spider_data_path_bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlIm57aRsVUh",
        "colab_type": "text"
      },
      "source": [
        "## Alter databases from Spider dataset\n",
        "\n",
        "```\n",
        "for each database\n",
        "  randomly delete COLUMN_DELETE_RATE of non primary keys columns\n",
        "  randomly delete ROW_DELETE_RATE of rows (TODO)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWjtp7TPvGGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# regenerate tables.json for original dataset\n",
        "%cd $spider_repo_path_bash\n",
        "!echo \"[]\" > $spider_data_path_bash/tables_empty.json\n",
        "!python2 preprocess/get_tables.py $original_databases_path_bash $dataset_tables $spider_data_path_bash/tables_empty.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VixJPUHnqvbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "COLUMN_DELETE_RATE = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edmOYAxE2lzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tables_dict(tables_file_path):\n",
        "  tables_dict = {}\n",
        "  with open(tables_file_path) as json_file:\n",
        "    tables = json.load(json_file)\n",
        "    for db_tables in tables:\n",
        "      db_id = db_tables['db_id']\n",
        "      tables_dict[db_id] = db_tables\n",
        "  return tables_dict\n",
        "\n",
        "tables_dict = get_tables_dict(dataset_tables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxg9zO1CqvZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_table_names(db_name, tables_dict):\n",
        "  return tables_dict[db_name]['table_names_original']\n",
        "\n",
        "def get_primary_keys_columns_names_and_types(db_name, table_name, tables_dict):\n",
        "  table_dict = tables_dict[db_name]\n",
        "  column_names = [\n",
        "                   {\n",
        "                     'table_name': table_name,\n",
        "                     'column_name': col[1],\n",
        "                     'type': table_dict['column_types'][idx],\n",
        "                     'uninterpreted_type': table_dict['column_uninterpreted_types'][idx],\n",
        "                     'foreign_key': None,\n",
        "                     'primary_key': True,\n",
        "                   } \n",
        "                   for idx, col in enumerate(table_dict['column_names_original'])\n",
        "                   if col[0]>=0\n",
        "                   and table_dict['table_names_original'][col[0]]==table_name\n",
        "                   and idx in table_dict['primary_keys']\n",
        "                 ]\n",
        "  return column_names\n",
        "\n",
        "def get_column_name(table_dict, column_index):\n",
        "  return table_dict['column_names_original'][column_index][1]\n",
        "\n",
        "def get_table_and_column_names(table_dict, column_index):\n",
        "  return {\n",
        "      'table': table_dict['table_names_original'][table_dict['column_names_original'][column_index][0]],\n",
        "      'column': get_column_name(table_dict, column_index),\n",
        "  }\n",
        "\n",
        "def get_non_primary_keys_columns_descriptions(db_name, table_name, tables_dict):\n",
        "  table_dict = tables_dict[db_name]\n",
        "  foreign_keys = { get_column_name(table_dict, fk[0]) : get_table_and_column_names(table_dict, fk[1])\n",
        "                    for fk in table_dict['foreign_keys']}\n",
        "  column_descriptions = [\n",
        "                   {\n",
        "                     'table_name': table_name,\n",
        "                     'column_name': column_name,\n",
        "                     'type': table_dict['column_types'][idx],\n",
        "                     'uninterpreted_type': table_dict['column_uninterpreted_types'][idx],\n",
        "                     'foreign_key': foreign_keys[column_name] if column_name in foreign_keys else None,\n",
        "                     'primary_key': False,\n",
        "                   } \n",
        "                   for idx, (table_index, column_name) in enumerate(table_dict['column_names_original'])\n",
        "                   if table_index>=0\n",
        "                      and table_dict['table_names_original'][table_index]==table_name\n",
        "                      and idx not in table_dict['primary_keys']\n",
        "                 ]\n",
        "  return column_descriptions\n",
        "\n",
        "def delete_columns(db, table_name, columns_to_keep):\n",
        "  names = ', '.join([col[\"column_name\"] for col in columns_to_keep])\n",
        "  names_and_types = ',\\n'.join([f'\"{col[\"column_name\"]}\" {col[\"uninterpreted_type\"]}' for col in columns_to_keep])\n",
        "  primary_keys = ', '.join(\n",
        "      [\n",
        "        f'\"{col[\"column_name\"]}\"'\n",
        "        for col in columns_to_keep\n",
        "        if col['primary_key']==True\n",
        "      ])\n",
        "  if len(primary_keys) > 0:\n",
        "    primary_keys = f',\\nprimary key({primary_keys})\\n'\n",
        "  foreign_keys = ',\\n'.join(\n",
        "      [\n",
        "        f'foreign key(\"{col[\"column_name\"]}\") references \"{col[\"foreign_key\"][\"table\"]}\"(\"{col[\"foreign_key\"][\"column\"]}\")'\n",
        "        for col in columns_to_keep\n",
        "        if col['foreign_key']!=None\n",
        "      ])\n",
        "  if len(foreign_keys) > 0:\n",
        "    foreign_keys = f',\\n{foreign_keys}\\n'\n",
        "  command = f\"CREATE TEMPORARY TABLE {table_name}_backup({names_and_types});\\n\" \\\n",
        "          + f\"INSERT INTO {table_name}_backup SELECT {names} FROM {table_name};\\n\" \\\n",
        "          + f\"DROP TABLE {table_name};\\n\" \\\n",
        "          + f\"CREATE TABLE \\\"{table_name}\\\" (\\n\" \\\n",
        "          + f\"{names_and_types}\" \\\n",
        "          + f\"{primary_keys}\" \\\n",
        "          + f\"{foreign_keys}\" \\\n",
        "          + f\");\\n\" \\\n",
        "          + f\"INSERT INTO {table_name} SELECT {names} FROM {table_name}_backup;\\n\" \\\n",
        "          + f\"DROP TABLE {table_name}_backup;\"\n",
        "  db.executescript(command)\n",
        "  db.commit()\n",
        "\n",
        "def randomly_delete_columns(db, db_name, tables_dict):\n",
        "    delete_columns_report = {'deleted_columns': []}\n",
        "    table_names = get_table_names(db_name, tables_dict)\n",
        "    for table_name in table_names:\n",
        "      columns_to_keep = get_non_primary_keys_columns_descriptions(db_name, table_name, tables_dict)\n",
        "      for column in columns_to_keep:\n",
        "        if random.random() < COLUMN_DELETE_RATE:\n",
        "          columns_to_keep.remove(column)\n",
        "          delete_columns_report['deleted_columns'].append(column)\n",
        "      columns_to_keep.extend(get_primary_keys_columns_names_and_types(db_name, table_name, tables_dict))\n",
        "      delete_columns(db, table_name, columns_to_keep)\n",
        "    return delete_columns_report\n",
        "\n",
        "def write_schema(db, out_file_path):\n",
        "  with open(out_file_path, 'w') as f:\n",
        "    for line in db.iterdump():\n",
        "      f.write('%s\\n' % line)\n",
        "\n",
        "def alter_database(db_name, database_folder_path, database_file_path, tables_dict):\n",
        "  with sqlite3.connect(database_file_path) as db:\n",
        "    original_schema_path = os.path.join(database_folder_path, 'original_schema.sql')\n",
        "    altered_schema_path = os.path.join(database_folder_path, 'schema.sql')\n",
        "    # dump_diff_path_bash = os.path.join(database_folder_path, 'dump.diff').replace(' ', '\\\\ ')\n",
        "    alteration_report_path = os.path.join(database_folder_path, 'alteration_report.json')\n",
        "\n",
        "    alterations_report = {}\n",
        "\n",
        "    # write schema before alteration\n",
        "    write_schema(db, original_schema_path)\n",
        "\n",
        "    alterations_report['delete_columns'] = randomly_delete_columns(db, db_name, tables_dict)\n",
        "    # TODO: randomly delete rows\n",
        "\n",
        "    # write schema after alteration\n",
        "    write_schema(db, altered_schema_path)\n",
        "\n",
        "    # write alterations report\n",
        "    with open(alteration_report_path, 'w') as outfile:\n",
        "      json.dump(alterations_report, outfile, indent=4)\n",
        "\n",
        "    # # print database diff\n",
        "    # !diff $original_dump_path $altered_dump_path > $dump_diff_path_bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RczD40Fse7-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clone original databases to altered databases path before altering them\n",
        "%rm -rf $altered_databases_path_bash\n",
        "%cp -r $original_databases_path_bash $altered_databases_path_bash\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "with tqdm(os.listdir(original_databases_path)) as db_it:\n",
        "  for database_folder in db_it:\n",
        "    db_it.set_postfix_str(database_folder)\n",
        "    database_folder_path = os.path.join(altered_databases_path, database_folder)\n",
        "    database_file_path = os.path.join(database_folder_path, f'{database_folder}.sqlite')\n",
        "    try:\n",
        "      alter_database(database_folder, database_folder_path, database_file_path, tables_dict)\n",
        "    except Exception as e:\n",
        "      tqdm.write(f'Error occured while processing database {database_folder}: {e}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6g1-56VzKXF",
        "colab_type": "text"
      },
      "source": [
        "## Identify questions which are affected by dataset alteration\n",
        "\n",
        "```\n",
        "for each dataset question and its SQL query translation\n",
        "  define_answerable_status(SQL query, altered target database)\n",
        "\n",
        "function define_answerable_status(SQL query, altered target database):\n",
        "  find out whether at least one of the columns deleted from the target database was used in the SELECT statement of the SQL query\n",
        "  find out whether at least one of the columns deleted from the target database was used in the WHERE statement of the SQL query\n",
        "  find out whether at least one of the columns deleted from the target database was used in the HAVING statement of the SQL query\n",
        "  find out whether at least one of the columns deleted from the target database was used in the FROM statement of the SQL query\n",
        "  find out the impact of row deletion (TODO)\n",
        "  if any column was used in the SELECT, HAVING, WHERE or FROM statement of the database:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0XEpVzswMmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "altered_spider_data_path_bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWg4aCOyd5h6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate tables.json for altered dataset\n",
        "%cd $spider_repo_path_bash\n",
        "!echo \"[]\" > $altered_spider_data_path_bash/tables_empty.json\n",
        "!python2 preprocess/get_tables.py $altered_databases_path $altered_dataset_tables_bash $altered_spider_data_path_bash/tables_empty.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5ogMcfnGIpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dev = pd.read_json(dev_path)\n",
        "df_train = pd.read_json(train_path)\n",
        "df_dev[\"split\"]=\"dev\"\n",
        "df_train[\"split\"]=\"train\"\n",
        "df = pd.concat([df_dev,df_train], ignore_index=True)\n",
        "df['column_not_available_for_select'] = None\n",
        "df['column_not_available_for_where'] = None\n",
        "df['column_not_available_for_having'] = None\n",
        "df['column_not_available_for_from'] = None\n",
        "\n",
        "tables_dict = get_tables_dict(dataset_tables)\n",
        "altered_tables_dict = get_tables_dict(altered_dataset_tables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doF6uUxHXeHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# description of sql_dict format: https://github.com/taoyds/spider/blob/master/preprocess/parsed_sql_examples.sql\n",
        "\n",
        "def get_column_full_names_from_indices(column_indices, db_tables):\n",
        "    full_names = [('_' if db_tables['column_names_original'][column_index][0]<0 else db_tables['table_names_original'][db_tables['column_names_original'][column_index][0]],\n",
        "             db_tables['column_names_original'][column_index][1])\n",
        "            for column_index in column_indices]\n",
        "    full_names = [full_name for full_name in full_names if full_name[1] != '*']\n",
        "    return set(full_names)\n",
        "\n",
        "def get_column_ids_used_in_col_unit(col_unit):\n",
        "    (_, col_id, _) = col_unit\n",
        "    return [col_id]\n",
        "\n",
        "def get_column_ids_used_in_val_unit(val_unit):\n",
        "    columns = set()\n",
        "    (_, col_unit1, col_unit2) = val_unit\n",
        "    for col_unit in [col_unit1, col_unit2]:\n",
        "        if col_unit:\n",
        "            columns.update(get_column_ids_used_in_col_unit(col_unit))\n",
        "    return columns\n",
        "\n",
        "def get_column_ids_used_in_condition(condition):\n",
        "    columns = set()\n",
        "    for cond_unit_and_or in condition:\n",
        "        if isinstance(cond_unit_and_or, list):\n",
        "            (_, _, val_unit, val1, val2) = cond_unit_and_or\n",
        "            columns.update(get_column_ids_used_in_val_unit(val_unit))\n",
        "            for val in [val1, val2]:\n",
        "                if isinstance(val, list):\n",
        "                    columns.add(val[1])\n",
        "    return columns\n",
        "\n",
        "def get_columns_used_in_select(sql_dict, db_tables):\n",
        "    columns = set()\n",
        "    _, selects = sql_dict['select']\n",
        "    for (_, val_unit) in selects:\n",
        "        columns.update(get_column_ids_used_in_val_unit(val_unit))\n",
        "    return get_column_full_names_from_indices(columns, db_tables)\n",
        "\n",
        "def get_columns_used_in_where(sql_dict, db_tables):\n",
        "    columns = set()\n",
        "    condition = sql_dict['where']\n",
        "    columns.update(get_column_ids_used_in_condition(condition))\n",
        "    return get_column_full_names_from_indices(columns, db_tables)\n",
        "\n",
        "def get_columns_used_in_having(sql_dict, db_tables):\n",
        "    columns = set()\n",
        "    condition = sql_dict['having']\n",
        "    columns.update(get_column_ids_used_in_condition(condition))\n",
        "    return get_column_full_names_from_indices(columns, db_tables)\n",
        "\n",
        "def get_columns_used_in_from(sql_dict, db_tables):\n",
        "    columns = set()\n",
        "    from_dict = sql_dict['from']\n",
        "    condition = from_dict['conds']\n",
        "    columns.update(get_column_ids_used_in_condition(condition))\n",
        "    return get_column_full_names_from_indices(columns, db_tables)\n",
        "\n",
        "def get_columns_from_db(db_tables):\n",
        "    columns = set(range(1, len(db_tables['column_names_original'])))\n",
        "    return get_column_full_names_from_indices(columns, db_tables)\n",
        "\n",
        "def get_columns_not_used_in_query(sql_dict, db_tables):\n",
        "    columns = get_columns_from_db(db_tables)\n",
        "    columns -= get_columns_used_in_select(sql_dict, db_tables)\n",
        "    columns -= get_columns_used_in_where(sql_dict, db_tables)\n",
        "    columns -= get_columns_used_in_having(sql_dict, db_tables)\n",
        "    columns -= get_columns_used_in_from(sql_dict, db_tables)\n",
        "    return get_column_full_names_from_indices(columns, db_tables)\n",
        "\n",
        "# test above functions on some examples\n",
        "for query_id in [1008, 1012, 1018, 1020, 1022]:\n",
        "    row = df.iloc[query_id]\n",
        "    print(query_id)\n",
        "    print(row.question)\n",
        "    print(row.query)\n",
        "    print('columns_used_in_select', get_columns_used_in_select(row.sql, tables_dict[row.db_id]))\n",
        "    print('columns_used_in_where', get_columns_used_in_where(row.sql, tables_dict[row.db_id]))\n",
        "    print('columns_used_in_having', get_columns_used_in_having(row.sql, tables_dict[row.db_id]))\n",
        "    print('columns_used_in_from', get_columns_used_in_from(row.sql, tables_dict[row.db_id]))\n",
        "    print('get_columns_from_db', get_columns_from_db(tables_dict[row.db_id]))\n",
        "    # print('columns_NOT_used_in_query', get_columns_not_used_in_query(row.sql, tables_dict[row.db_id]))\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bxDp1nmgleT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_column_not_available(df, index, altered_tables_dict, columns_used, label_name):\n",
        "  row = df.iloc[index]\n",
        "  columns_from_db = get_columns_from_db(altered_tables_dict[row.db_id])  \n",
        "  for column_used in columns_used:\n",
        "    if column_used not in columns_from_db:\n",
        "      df.loc[index, label_name] = True\n",
        "      return      \n",
        "  df.loc[index, label_name] = False\n",
        "  return\n",
        "\n",
        "def set_column_not_available_for_select(df, index, tables_dict, altered_tables_dict):\n",
        "  row = df.iloc[index]\n",
        "  columns_used_in_select = get_columns_used_in_select(row.sql, tables_dict[row.db_id])\n",
        "  set_column_not_available(df, index, altered_tables_dict, columns_used_in_select, 'column_not_available_for_select')\n",
        "\n",
        "def set_column_not_available_for_where(df, index, tables_dict, altered_tables_dict):\n",
        "  row = df.iloc[index]\n",
        "  columns_used_in_where = get_columns_used_in_where(row.sql, tables_dict[row.db_id])\n",
        "  set_column_not_available(df, index, altered_tables_dict, columns_used_in_where, 'column_not_available_for_where')\n",
        "\n",
        "def set_column_not_available_for_having(df, index, tables_dict, altered_tables_dict):\n",
        "  row = df.iloc[index]\n",
        "  columns_used_in_having = get_columns_used_in_having(row.sql, tables_dict[row.db_id])\n",
        "  set_column_not_available(df, index, altered_tables_dict, columns_used_in_having, 'column_not_available_for_having')\n",
        "\n",
        "def set_column_not_available_for_from(df, index, tables_dict, altered_tables_dict):\n",
        "  row = df.iloc[index]\n",
        "  columns_used_in_from = get_columns_used_in_from(row.sql, tables_dict[row.db_id])\n",
        "  set_column_not_available(df, index, altered_tables_dict, columns_used_in_from, 'column_not_available_for_from')\n",
        "\n",
        "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "  set_column_not_available_for_select(df, index, tables_dict, altered_tables_dict)\n",
        "  set_column_not_available_for_where(df, index, tables_dict, altered_tables_dict)\n",
        "  set_column_not_available_for_having(df, index, tables_dict, altered_tables_dict)\n",
        "  set_column_not_available_for_from(df, index, tables_dict, altered_tables_dict)\n",
        "\n",
        "df[\"answerability\"] = ~df[[c for c in df.columns if \"column_not_available_\" in c]].max(axis=1).map(bool)\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBAeceQmz_S1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_alteration_report(db_id):\n",
        "  alteration_report_path = os.path.join(altered_databases_path, db_id, 'alteration_report.json')\n",
        "  alteration_report = {}\n",
        "  with open(alteration_report_path) as json_file:\n",
        "    alteration_report = json.load(json_file)\n",
        "  return alteration_report\n",
        "\n",
        "def print_consequence_of_alteration(i, df):\n",
        "  db_id = df.loc[i, 'db_id']\n",
        "  alteration_report = read_alteration_report(db_id)\n",
        "  print(f'query index: {i}')\n",
        "  print(f'NL question: {df.loc[i, \"question\"]}')\n",
        "  print(f'SQL query: {df.loc[i, \"query\"]}')\n",
        "  print(f'Alterations on target DB ({db_id}):')\n",
        "  print(f' - deleted columns: {\", \".join([col[\"table_name\"]+\".\"+col[\"column_name\"] for col in alteration_report[\"delete_columns\"][\"deleted_columns\"]])}')\n",
        "  print(f'Consequences:')\n",
        "  print(f' - column_not_available_for_select: {df.loc[i, \"column_not_available_for_select\"]}')\n",
        "  print(f' - column_not_available_for_where:  {df.loc[i, \"column_not_available_for_where\"]}')\n",
        "  print(f' - column_not_available_for_having: {df.loc[i, \"column_not_available_for_having\"]}')\n",
        "  print(f' - column_not_available_for_from:   {df.loc[i, \"column_not_available_for_from\"]}')\n",
        "  print(f' -> {\"NOT\" if not df.loc[i, \"answerability\"] else \"\"} ANSWERABLE')\n",
        "\n",
        "indices = [0, 1, 8030, 8031, 8032]\n",
        "for i in indices:\n",
        "  print_consequence_of_alteration(i, df)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB2Pq_2az014",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ratio of answerable questions\n",
        "df.answerability.value_counts()[True]/len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gzaOB3e0j7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(altered_train_path, 'w') as outfile:\n",
        "  json.dump(json.loads(df[df[\"split\"]==\"train\"].to_json(orient='records')), outfile, indent=4)\n",
        "  \n",
        "with open(altered_dev_path, 'w') as outfile:\n",
        "  json.dump(json.loads(df[df[\"split\"]==\"dev\"].to_json(orient='records')), outfile, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjXZ3RVAXQJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!du -hcs $altered_spider_data_path_bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckDD_NgL37Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd $base_path\n",
        "!7z -a idk_dataset.7z $altered_spider_data_path_bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOOEO43r4EZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!du -h idk_dataset.7z"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}